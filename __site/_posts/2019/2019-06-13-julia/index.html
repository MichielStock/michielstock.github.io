<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
     <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
    <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/tufte.css">
<link rel="stylesheet" href="/css/latex.css">
<link rel="stylesheet" href="/css/adjust.css"> <!-- sheet to overwrite some clashing styles -->
<link rel="icon" href="/assets/favicon.png">

     <title>Automatic uncertainty propagation</title>  
</head>

<body>
<div id="layout">
  <div id="menu">
    <ul>
      <li><a href="/">Home</a></li>
      <li><a href="/CV/">CV</a></li>
      <li><a href="/research/">Research</a></li>
      <li><a href="/blog/">Blog</a></li>
      <li><a href="/tags/">Tags</a></li>
    </ul>
  </div>
  <div id="main">



<!-- Content appended here -->
<div class="franklin-content"><p><hr /> layout: &quot;post&quot; title: Uncertainty propagation and high-dimensional indexing in Julia date: &quot;2019-06-13&quot; –-</p>
<p><img src="../../images/2019_julia/julia.svg" alt="Don&#39;t at me like that python&#33; Julia just gets me&#33; It is more than a performance thing&#33;" /></p>
<p>I have been playing around with the <a href="https://julialang.org">Julia language</a> for about a year now &#40;just before the stable version was released&#41;. During that year, I&#39;ve been converted from a skeptic &#40;did not catch the bug during my stay at the <a href="https://poisotlab.io/">Poisotlab</a>&#41; to an enthousiast&#33; It is not only about the speed of native Julia code, though this alone would make it worth learning, but just the general pleasure of programming in this language. The type system central to Julia is just something which clicks to me as a scientist in a way that, for example, classes never clicked with me. Here I would like to show two case studies, relevant to the scientific programmer, which I think really demonstrate how natural it is to extend Julia and write powerful, generic code.</p>
<h2 id="automatic_uncertainty_propagation"><a href="#automatic_uncertainty_propagation">Automatic uncertainty propagation</a></h2>
<p>During my first year of college, I really liked the physics lectures because everything was exact. In my first year of physics, I really disliked the physics labs, because everything was messy. First-year physics labs are generally disliked partially because of the uninspiring topics &#40;measuring resistance in a wire&#33; determining a heat transfer coefficient&#33; measuring chirality of sugar&#33;&#41; and partly because this was the only lab in which &#39;exact&#39; measurements had to be performed. These labs introduced the complex and unexplained rules for measurement error propagation. Without any prior statistics or probability courses, it was lost on me where these strange rules originated from, as sharp contrast to nearly everything else in physics.</p>
<p>A year later, after plowing through a basic probability course, error propagation is less mysterious. Almost all these rules to account for the error can be derived from a simple principle:</p>
<blockquote>
<p>Given two <strong>independent</strong> random variables </p>
</blockquote>
\[X\]
<p>and </p>
\[Y\]
<p>, the variance of a linear combination </p>
\[\text{Var}[aX+bY]=a^2\text{Var}[X] + b^2\text{Var}[Y]\]
<p>.</p>
<p>Measurement errors are usually given by a standard deviation, the square root of the variance. Given this principle, error propagation is merely bookkeeping of the standard error on the measurement for various computations.</p>
<p>Instead of processing the measurements and the standard error separately, suppose we could just make a new type of number which contains both the observed value and its uncertainty. And suppose we could just compute something with these numbers, plugging them into our formulas where the error is automatically accounted for using the standard error propagation rules. In Julia, it is dead simple to construct such new numbers and just overload existing functions such that they compute in the correct way.</p>
<p>Let us make make a new type of structure <code>Measurement</code> which holds a value <code>x</code> and a &#40;positive&#41; measurement error <code>σ</code>.</p>
<pre><code class="language-julia">struct Measurement
    x::Real
    σ::Real
    function Measurement&#40;x, σ&#41;
        if σ &lt; zero&#40;σ&#41;
            error&#40;&quot;Measurement error should be non-zero&quot;&#41;
        end
        new&#40;x, σ&#41;
    end
end</code></pre>
<p>We also define a function to extract the value and error from instances of the type <code>Measurement</code>. This is similar to how the real and imaginary part of a complex number are extracted. Furthermore, we modify Julia&#39;s behavior such that instances are printed as <code>x ± σ</code>.</p>
<pre><code class="language-julia">val&#40;m::Measurement&#41; &#61; m.x;
err&#40;m::Measurement&#41; &#61; m.σ;</code></pre>
<p>Base.show&#40;io::IO, measurement::Measurement&#41; &#61; print&#40;io, &quot;&#36;&#36;&#40;measurement.x&#41; ± &#36;&#36;&#40;measurement.σ&#41;&quot;&#41;;</p>
<p>An intuitive syntax is to make a new binary operator <code>±</code> that adds the measurement error to a measured value, thus creating a <code>Measurement</code> instance.</p>
<pre><code class="language-julia">±&#40;x, σ&#41; &#61; Measurement&#40;x, σ&#41;;</code></pre>
<pre><code class="language-julia">m &#61; 9.0 ± 0.3</code></pre>
<p>9.0 ± 0.3</p>
<p>Now that we constructed the <code>Measurement</code> type, let us tell Julia how to compute with it. We can easily implement the standard <a href="https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulae">uncertainty propagation rules</a>.</p>
<pre><code class="language-julia"># scalar multiplication
Base.:*&#40;a::Real, m::Measurement&#41; &#61; a * m.x ± abs&#40;a&#41; * m.σ;
Base.:/&#40;m::Measurement, a::Real&#41; &#61; inv&#40;a&#41; * m;
# adding and substracting measurements
Base.:&#43;&#40;m1::Measurement, m2::Measurement&#41; &#61; &#40;m1.x &#43; m2.x&#41; ± √&#40;m1.σ^2 &#43; m2.σ^2&#41;;
Base.:-&#40;m1::Measurement, m2::Measurement&#41; &#61; &#40;m1.x - m2.x&#41; ± √&#40;m1.σ^2 &#43; m2.σ^2&#41;;
Base.:-&#40;m::Measurement&#41; &#61; -m.x ± m.σ;
# adding a constant
Base.:&#43;&#40;m::Measurement, a::Real&#41; &#61; m &#43; &#40;a ± zero&#40;a&#41;&#41;;
Base.:&#43;&#40;a::Real, m::Measurement&#41; &#61; m &#43; a;
# multiplying two measurments
Base.:*&#40;m1::Measurement, m2::Measurement&#41; &#61; m1.x * m2.x ± &#40;m1.x * m2.x&#41; * √&#40;&#40;m1.σ / m1.x&#41;^2 &#43; &#40;m2.σ / m2.x&#41;^2&#41;;</code></pre>
<p>For example:</p>
<pre><code class="language-julia">2m</code></pre>
<p>18.0 ± 0.6</p>
<pre><code class="language-julia">m &#43; 1.2</code></pre>
<p>10.2 ± 0.3</p>
<pre><code class="language-julia">&#40;12 ± 0.3&#41; * &#40;18 ± 0.8&#41;</code></pre>
<p>216 ± 11.014535850411491</p>
<pre><code class="language-julia">m / 4</code></pre>
<p>2.25 ± 0.075</p>
<p>For nonlinear functions, we can compute an approximate uncertainty propagation using a first-order Taylor approximation. We have, for any function </p>
\[f(x)\]
<p>:</p>
\[
f(x\pm \sigma) \approx f(x) \pm |f'(x)|\sigma\,.
\]
<p>For example, for squaring a function, we have</p>
\[
(x\pm\sigma)^2 = x^2 \pm 2|x|\sigma\,.
\]
<p>Note that this is consistent with the above rules for multiplication. Let us implement the general formula for raising a measurement to the power </p>
\[p\]
<p>.</p>
<pre><code class="language-julia">Base.:^&#40;m::Measurement, p::Integer&#41; &#61; m.x^2 ± abs&#40;p*m.x^&#40;p-1&#41; * m.σ&#41;;</code></pre>
<pre><code class="language-julia">m^2</code></pre>
<p>81.0 ± 5.3999999999999995</p>
<p>We can implement this for all the standard mathematical functions one by one. However, Julia provides use with two efficient tools to do this in one swoop: automatic differentiation and metaprogramming. We just loop a list of functions of interest and automatically generate the correct approximate rule.</p>
<pre><code class="language-julia">using ForwardDiff

for f in &#91;:sin, :cos, :tan, :exp, :log, :log2, :log10, :sqrt, :inv&#93;
    eval&#40;quote
        # this is a line of code generated using string interpolation
        Base.&#36;f&#40;m::Measurement&#41; &#61; &#36;f&#40;m.x&#41; ± abs&#40;ForwardDiff.derivative&#40;&#36;f, m.x&#41; * m.σ&#41;
    end&#41;
end</code></pre>
<pre><code class="language-julia">cos&#40;m&#41;</code></pre>
<p>-0.9111302618846769 ± 0.12363554557252697</p>
<pre><code class="language-julia">log&#40;m^2&#41;</code></pre>
<p>4.394449154672439 ± 0.06666666666666665</p>
<pre><code class="language-julia">2log&#40;m&#41;  # same</code></pre>
<p>4.394449154672439 ± 0.06666666666666667</p>
<p>Let&#39;s apply this to in a somewhat realistic setting. Many methods in analytical chemistry are based on the law of <a href="https://en.wikipedia.org/wiki/Beer&#37;E2&#37;80&#37;93Lambert_law">Beer-Lambert</a>. This law relates the absorption of a ray of light passing through a cuvet with the concentration of a solution. For a given reference intensity </p>
\[I_0\]
<p>at concentration of 0 and a lower intensity </p>
\[I\]
<p>when it passes through a solution of concentration </p>
\[c\]
<p>, we have</p>
\[
\log \left(\frac{I_0}{I}\right) = \varepsilon c l\,,
\]
<p>with </p>
\[\varepsilon\]
<p>the molar extinction coefficient and </p>
\[l\]
<p>the thickness of the cuvet.</p>
<p>Suppose we want to determine the extinction coefficient for some substance, using a cuvet of thickness </p>
\[0.02\pm 0.001\]
<p>and a reference solution of a concentration of </p>
\[0.73\pm 0.02\]
<p>M.</p>
<pre><code class="language-julia">c &#61; 0.72 ± 0.02
l &#61; 0.02 ± 1e-4</code></pre>
<p>0.02 ± 0.0001</p>
<p>We perform some intensity measurements, with associated measurement errors.</p>
<pre><code class="language-julia">I0 &#61; &#91;0.8 ± 0.07, 1.1 ± 0.11, 1.2 ± 0.08&#93;
I1 &#61; &#91;0.2 ± 0.12, 0.3 ± 0.08, 0.2 ± 0.3&#93;</code></pre>
<p>3-element Array&#123;Measurement,1&#125;:
 0.2 ± 0.12
 0.3 ± 0.08
 0.2 ± 0.3</p>
<p>Some straightforward manipulations of the Beer-Lambert formula allows us to compute an estimate of </p>
\[\varepsilon\]
<p>. Since the estimator only uses operations which allow the uncertainty propagation, the estimate directly gives the value with the standard error.</p>
<pre><code class="language-julia">mean&#40;x&#41; &#61; sum&#40;x&#41; / length&#40;x&#41;
ε &#61; log&#40;mean&#40;I0&#41; * inv&#40;mean&#40;I1&#41;&#41;&#41; * inv&#40;c&#41; * inv&#40;l&#41;</code></pre>
<p>103.33868440484952 ± 33.327646598494475</p>
<p>We can then subsequently use this molecular extinction in further computations, for example to estimate new concentrations, directly with confidence intervals&#33;</p>
<h2 id="general_n_-dimensional_convolutions"><a href="#general_n_-dimensional_convolutions">General </p>
\[n\]
<p>-dimensional convolutions</a></h2>
<p>The flexibility of Julia makes it easy to write general, but highly optimized code. One aspect, I really like is its indexing system. When you get the hang of it, you will never complain again about the 1-based indexing&#33;</p>
<p>Consider an array:</p>
<pre><code class="language-julia">A &#61; rand&#40;3, 5&#41;</code></pre>
<p>3×5 Array&#123;Float64,2&#125;:
 0.270625  0.397293  0.348364   0.241978   0.424326
 0.850476  0.213356  0.609399   0.0948646  0.665579
 0.789739  0.435704  0.0511797  0.211703   0.586431</p>
<p>Julia&#39;s arrays are column major, rather than row major as in Numpy. This means that you should see a matrix as a set of columns, not a set of rows. This is important, when processing matrices, as processing matrices row by row can be substantially slower than processing them column by column.</p>
<pre><code class="language-julia">using BenchmarkTools

function sum_row_by_row&#40;A&#41;
    result &#61; 0.0
    for i in 1:size&#40;A, 1&#41;
        for j in 1:size&#40;A, 2&#41;
            result &#43;&#61; A&#91;i,j&#93;
        end
    end
    return result
end

function sum_col_by_col&#40;A&#41;
    result &#61; 0.0
    for j in 1:size&#40;A, 2&#41;
        for i in 1:size&#40;A, 1&#41;
            result &#43;&#61; A&#91;i,j&#93;
        end
    end
    return result
end

B &#61; randn&#40;5000, 5000&#41;;  # bigger matrix to illustrate</code></pre>
<pre><code class="language-julia">@btime sum_row_by_row&#40;&#36;&#36;B&#41;</code></pre>
<p>  164.997 ms &#40;0 allocations: 0 bytes&#41;





6479.463039431909</p>
<pre><code class="language-julia">@btime sum_col_by_col&#40;&#36;&#36;B&#41;</code></pre>
<p>  27.304 ms &#40;0 allocations: 0 bytes&#41;





6479.463039430924</p>
<p>Not a bad speedup by just switching the order of the two loops&#33; The order of the elements can be accessed using <code>LinearIndices&#40;A&#41;</code>. This will generate a matrix of the same size of <code>A</code>, containing the index of all elements.</p>
<pre><code class="language-julia">LinearIndices&#40;A&#41;</code></pre>
<p>3×5 LinearIndices&#123;2,Tuple&#123;Base.OneTo&#123;Int64&#125;,Base.OneTo&#123;Int64}}&#125;:
 1  4  7  10  13
 2  5  8  11  14
 3  6  9  12  15</p>
<p>When indexing over an array, such an object can be generated and used to process the array. It seems incredible wasteful, but upon compiling the function this is completely optimized away, so it has no memory footprint&#33;</p>
<pre><code class="language-julia">function sum_lin_ind&#40;A&#41;
    L &#61; LinearIndices&#40;A&#41;
    result &#61; 0.0
    for I in L
        result &#43;&#61; A&#91;I&#93;
    end
    return result
end

@btime sum_lin_ind&#40;&#36;&#36;B&#41;</code></pre>
<p>  27.468 ms &#40;0 allocations: 0 bytes&#41;





6479.463039430924</p>
<p>In many cases, <code>CartesianIndices&#40;A&#41;</code> is more useful, this will generate an array of the same size, but containing the <code>CartesianIndex</code> instances. These hold the indices of the corresponding elements and are in general easier to work with.</p>
<pre><code class="language-julia">CartesianIndices&#40;A&#41;</code></pre>
<p>3×5 CartesianIndices&#123;2,Tuple&#123;Base.OneTo&#123;Int64&#125;,Base.OneTo&#123;Int64}}&#125;:
 CartesianIndex&#40;1, 1&#41;  CartesianIndex&#40;1, 2&#41;  …  CartesianIndex&#40;1, 5&#41;
 CartesianIndex&#40;2, 1&#41;  CartesianIndex&#40;2, 2&#41;     CartesianIndex&#40;2, 5&#41;
 CartesianIndex&#40;3, 1&#41;  CartesianIndex&#40;3, 2&#41;     CartesianIndex&#40;3, 5&#41;</p>
<p>The important thing the note is that this all works agnostic on the dimensionality of the arrays. Whether <code>A</code> has one, two, three of </p>
\[n\]
<p>dimensions, it all works the same.</p>
<p>I will illustrate this by writing a small piece of code for performing a general convolution, regardless of the dimensionality of <code>A</code>. With minor modifications, this can be a versatile building block for many interesting numerical algorithms.</p>
<pre><code class="language-julia">function generalconvolve&#33;&#40;A::AbstractArray, out, W, f&#61;&#40;v,a&#41;-&gt;v&#41;
    @assert size&#40;A&#41; &#61;&#61; size&#40;out&#41;
    @assert all&#40;size&#40;W&#41; .&#37; 2 .&#61;&#61; 1&#41;  # all dimensions of W odd
    C &#61; CartesianIndices&#40;A&#41;
    Ifirst, Ilast &#61; first&#40;C&#41;, last&#40;C&#41;
    I1 &#61; oneunit&#40;Ifirst&#41;
    width &#61; CartesianIndices&#40;W&#41;&#91;div&#40;length&#40;W&#41;&#43;1, 2&#41;&#93; - I1
    for I in &#40;Ifirst&#43;width&#41;:&#40;Ilast-width&#41;
        value &#61; 0.0
        for &#40;J, w&#41; in zip&#40;&#40;I-width&#41;:&#40;I&#43;width&#41;, W&#41;
            value &#43;&#61; w * A&#91;J&#93;
        end
        out&#91;I&#93; &#61; f&#40;value, A&#91;I&#93;&#41;
    end
    return out
end

function generalconvolve&#40;A::AbstractArray, W, f&#61;&#40;v,a&#41;-&gt;v&#41;
    out &#61; copy&#40;A&#41;
    return generalconvolve&#33;&#40;A, out, W, f&#41;
end</code></pre>
<p>generalconvolve &#40;generic function with 2 methods&#41;</p>
<p>This function has as inputs:</p>
<ul>
<li><p><code>A</code>: a general array;</p>
</li>
<li><p><code>W</code>: a smaller array with the same number of dimensions to do the convolution;</p>
</li>
<li><p><code>f</code>: a function to transform the convolved values, with the convolution result and the original value as input. The identity map is the result, but this can be used for more complex methods.</p>
</li>
</ul>
<h3 id="kernel_smoothing"><a href="#kernel_smoothing">Kernel smoothing</a></h3>
<p>Let us start with a basic example from statistics: <a href="https://en.wikipedia.org/wiki/Kernel_smoother">kernel smoothing</a>. In the most simple case, we have some noisy vector </p>
\[\mathbf{x}\]
<p>and we want to denoise it by taking a moving window average: the value at every point is replaced by a local average. Given a bandwidth of </p>
\[p\]
<p>&#40;taking the average of the elements </p>
\[x_{i-p}, x_{i-p+1},\ldots x_{i-1}, x_{i}, x_{i+1},\ldots, x_{i+p-1}, x_{i+p}\]
<p>for at every position </p>
\[i\]
<p>&#41; is done by setting an appropriate value of <code>W</code>.</p>
<pre><code class="language-julia">kernelsmoothing&#40;x::AbstractVector; p::Int&#61;3&#41; &#61; generalconvolve&#40;x, ones&#40;2p&#43;1&#41; / &#40;2p&#43;1&#41;&#41;;</code></pre>
<pre><code class="language-julia"># generate a noisy signal
t &#61; 0:0.01:3
x &#61; exp.&#40;t&#41; .* sin.&#40;t&#41; &#43; randn&#40;length&#40;t&#41;&#41;;</code></pre>
<pre><code class="language-julia">using Plots

plot&#40;t, x, label&#61;&quot;signal&quot;&#41;
plot&#33;&#40;t, kernelsmoothing&#40;x, p&#61;10&#41;, label&#61;&quot;smoothed signal&quot;&#41;
xlabel&#33;&#40;&quot;t&quot;&#41;</code></pre>
<p><img src="../../images/2019_julia/output_51_0.png" alt="plot" /></p>
<p>Of course, other kernels can be used just as easily, as long as <code>W</code> is a proper probability density function. This way, a decayed importance can be placed on values farther from the center.</p>
<h3 id="image_gradients"><a href="#image_gradients">Image gradients</a></h3>
<p>Moving to two dimensions we can implement some basic image processing&#33; To compute the gradient of an image &#40;i.e. how the pixel values change locally in the </p>
\[x\]
<p>or </p>
\[y\]
<p>direction we just use the correct convolution.</p>
<pre><code class="language-julia">image_grad_x&#40;image::Matrix&#41; &#61; generalconvolve&#40;image, &#91;-1 0 1; -1 0 1; -1 0 1&#93; / 6&#41;&#91;2:end-1,2:end-1&#93;;
image_grad_y&#40;image::Matrix&#41; &#61; generalconvolve&#40;image, &#91;-1 -1 -1; 0 0 0; 1 1 1&#93; / 6&#41;&#91;2:end-1,2:end-1&#93;;</code></pre>
<pre><code class="language-julia">A &#61; reshape&#40;&#91;2.5x - 0.1y^2 &#43; 2*x*y for x in 1:7 for y in 1:10&#93;, 10, 7&#41;</code></pre>
<p>10×7 Array&#123;Float64,2&#125;:
  4.4   8.9  13.4  17.9   22.4   26.9   31.4
  6.1  12.6  19.1  25.6   32.1   38.6   45.1
  7.6  16.1  24.6  33.1   41.6   50.1   58.6
  8.9  19.4  29.9  40.4   50.9   61.4   71.9
 10.0  22.5  35.0  47.5   60.0   72.5   85.0
 10.9  25.4  39.9  54.4   68.9   83.4   97.9
 11.6  28.1  44.6  61.1   77.6   94.1  110.6
 12.1  30.6  49.1  67.6   86.1  104.6  123.1
 12.4  32.9  53.4  73.9   94.4  114.9  135.4
 12.5  35.0  57.5  80.0  102.5  125.0  147.5</p>
<pre><code class="language-julia">heatmap&#40;A&#41;</code></pre>
<p><img src="../../images/2019_julia/output_56_0.png" alt="plot" /></p>
<pre><code class="language-julia">Ax &#61; image_grad_x&#40;A&#41;</code></pre>
<p>8×5 Array&#123;Float64,2&#125;:
  6.5   6.5   6.5   6.5   6.5
  8.5   8.5   8.5   8.5   8.5
 10.5  10.5  10.5  10.5  10.5
 12.5  12.5  12.5  12.5  12.5
 14.5  14.5  14.5  14.5  14.5
 16.5  16.5  16.5  16.5  16.5
 18.5  18.5  18.5  18.5  18.5
 20.5  20.5  20.5  20.5  20.5</p>
<pre><code class="language-julia">heatmap&#40;Ax&#41;</code></pre>
<p><img src="../../images/2019_julia/output_58_0.png" alt="plot" /></p>
<pre><code class="language-julia">Ay &#61; image_grad_y&#40;A&#41;</code></pre>
<p>8×5 Array&#123;Float64,2&#125;:
 3.6  5.6  7.6  9.6  11.6
 3.4  5.4  7.4  9.4  11.4
 3.2  5.2  7.2  9.2  11.2
 3.0  5.0  7.0  9.0  11.0
 2.8  4.8  6.8  8.8  10.8
 2.6  4.6  6.6  8.6  10.6
 2.4  4.4  6.4  8.4  10.4
 2.2  4.2  6.2  8.2  10.2</p>
<pre><code class="language-julia">heatmap&#40;Ay&#41;</code></pre>
<p><img src="../../images/2019_julia/output_60_0.png" alt="plot" /></p>
<p>We can directly apply this a real image, and it works&#33;</p>
<pre><code class="language-julia">using TestImages

img &#61; convert&#40;Array&#123;Float64&#125;, testimage&#40;&quot;cameraman&quot;&#41;&#41;&#91;end:-1:1,:&#93;
heatmap&#40;img&#41;</code></pre>
<p><img src="../../images/2019_julia/output_62_0.png" alt="plot" /></p>
<pre><code class="language-julia">heatmap&#40;image_grad_x&#40;img&#41;&#41;</code></pre>
<p><img src="../../images/2019_julia/output_63_0.png" alt="plot" /></p>
<pre><code class="language-julia">heatmap&#40;image_grad_y&#40;img&#41;&#41;</code></pre>
<p><img src="../../images/2019_julia/output_64_0.png" alt="plot" /></p>
<h3 id="game_of_life_a_cellular_automaton"><a href="#game_of_life_a_cellular_automaton">Game of Life: a cellular automaton</a></h3>
<p>Something different, our piece of code can be hacked to perform the update for <a href="https://en.wikipedia.org/wiki/Conway&#37;27s_Game_of_Life">Conway&#39;s Game of Life</a>. This is a simple cellular automata defined on a 2-D grid with binary states, a cell can either be alive or dead. The rules for updating a cell&#39;s state from one step to the next follows the following simple rules:</p>
<ul>
<li><p>a living cell with one or two living neighbors dies &#40;underpopulation&#41;;</p>
</li>
<li><p>a living cell with four or more living neighbors dies &#40;overpopulation&#41;;</p>
</li>
<li><p>a living cell with two or three living neighbors remains alive &#40;survival&#41;;</p>
</li>
<li><p>a dead cell with exactly three living neighbors turns alive &#40;procreation&#41;.</p>
</li>
</ul>
<p>We choose <code>W</code> such that the living neighbors are counted. <code>f</code> is chosen to represent the above updating rules.</p>
<pre><code class="language-julia">gol_rules&#40;nneighbors, state&#41; &#61; &#40;state &amp;&amp; 2 &lt;&#61; nneighbors &lt;&#61; 3&#41; || &#40;&#33;state &amp;&amp; nneighbors &#61;&#61; 3&#41;;
gol_update&#40;A::AbstractMatrix&#123;Bool&#125;&#41; &#61; generalconvolve&#40;A, &#91;1 1 1; 1 0 1; 1 1 1&#93;, gol_rules&#41;;</code></pre>
<pre><code class="language-julia">A &#61; rand&#40;Bool, 100, 100&#41;;
heatmap&#40;A&#41;</code></pre>
<p><img src="../../images/2019_julia/output_67_0.png" alt="plot" /></p>
<pre><code class="language-julia">@btime A &#61; gol_update&#40;&#36;&#36;A&#41;  # quite fast&#33;
heatmap&#40;A&#41;</code></pre>
<p>  220.606 μs &#40;2 allocations: 10.09 KiB&#41;</p>
<p><img src="../../images/2019_julia/output_68_1.png" alt="plot" /></p>
<p>Simulate 1000 time steps.</p>
<pre><code class="language-julia">for i in 1:1000
    A &#61; gol_update&#40;A&#41;
end
heatmap&#40;A&#41;</code></pre>
<p><img src="../../images/2019_julia/output_70_0.png" alt="plot" /></p>
<p>We see that all the random states have been replaced by simple stationary structures and oscillators. Note that the edges are not updated because no correct boundary conditions are implemented.</p>
<h3 id="diffusion_of_ink_in_3d"><a href="#diffusion_of_ink_in_3d">Diffusion of ink in 3D</a></h3>
<p>As an example in 3D, consider a volume containing some spots of ink. Using a Gaussian convolution, we can simulate how these drops have diffused after some time.</p>
<p>First, we generate a matrix with ink drops at random places.</p>
<pre><code class="language-julia">A &#61; zeros&#40;100, 100, 100&#41;
A&#91;rand&#40;CartesianIndices&#40;A&#41;, 20&#41;&#93; .&#61; 10.0;  # 20 drops of ink at random spots</code></pre>
<p>The diffusion rate is determined by <a href="https://en.wikipedia.org/wiki/Fick&#37;27s_laws_of_diffusion">Fick&#39;s law</a>, for the sake of simplicity, we will simulate this using a simple Gaussian convolution in three dimensions.</p>
<pre><code class="language-julia">W &#61; Array&#123;Float64&#125;&#40;undef, 5, 5, 5&#41;

for I in CartesianIndices&#40;W&#41;
    i, j, k &#61; Tuple&#40;I&#41;
    W&#91;i,j,k&#93; &#61; exp&#40;-&#40;i-3&#41;^2 - &#40;j-3&#41;^2 - &#40;k-3&#41;^2&#41;
end

# normalize
W ./&#61; sum&#40;W&#41;;</code></pre>
<pre><code class="language-julia">nsteps &#61; 50
Anew &#61; similar&#40;A&#41;

for step in 1:nsteps
    generalconvolve&#33;&#40;A, Anew, W&#41;
    A, Anew &#61; Anew, A
end</code></pre>
<p>By taking a slice, we can see how the ink has spread.</p>
<pre><code class="language-julia">heatmap&#40;A&#91;31, :, :&#93;&#41;</code></pre>
<p><img src="../../images/2019_julia/output_78_0.png" alt="plot" /></p>
<h3 id="piecing_it_together_convoluton_with_uncertainty"><a href="#piecing_it_together_convoluton_with_uncertainty">Piecing it together: convoluton with uncertainty</a></h3>
<p>We developed a structure for dealing with a uncertainty propagation and a general convolution function. Can these two seemingly unrelated concepts play together? Without any problem&#33;</p>
<p>Consider a noise 1-D signal, with an uncertainty proportional to the absolute value of the signal. We first want to do a simple convolution similar to the kernel smoothing of earlier. Finally, we pass the signal to a sigmoid function, such that it is squeezed into the </p>
\[[0,1]\]
<p>interval.</p>
<pre><code class="language-julia">z &#61; 8exp.&#40;-0.4t&#41;.*sin.&#40;2t&#41;
z &#61; z .&#43; 0.1randn&#40;length&#40;t&#41;&#41; .± &#40;0.2abs.&#40;z&#41;.&#43; 0.1&#41;;</code></pre>
<pre><code class="language-julia">plot&#40;val.&#40;z&#41;, color&#61;&quot;blue&quot;, label&#61;&quot;z&quot;&#41;
plot&#33;&#40;val.&#40;z&#41; &#43; 2err.&#40;z&#41;, color&#61;&quot;red&quot;, label&#61;&quot;upper bound&quot;&#41;
plot&#33;&#40;val.&#40;z&#41; - 2err.&#40;z&#41;, color&#61;&quot;red&quot;, label&#61;&quot;lower bound&quot;&#41;</code></pre>
<p><img src="../../images/2019_julia/output_81_0.png" alt="plot" /></p>
<pre><code class="language-julia">sigmoid&#40;x, a&#61;0.0&#41; &#61; inv&#40;exp&#40;-x&#41; &#43; 1.0&#41;;</code></pre>
<pre><code class="language-julia">conv_sofmax_signal&#40;z::Vector; p::Int&#61;5&#41; &#61; generalconvolve&#40;z, ones&#40;2p&#43;1&#41;/&#40;2p&#43;1&#41;, &#40;x,a&#41;-&gt;inv&#40;exp&#40;-1x&#41; &#43; 1.0&#41;&#41;&#91;p&#43;1:end-p-1&#93;;</code></pre>
<pre><code class="language-julia">p &#61; conv_sofmax_signal&#40;z&#41;;
plot&#40;val.&#40;p&#41;, color&#61;&quot;blue&quot;, label&#61;&quot;processed signal&quot;&#41;
plot&#33;&#40;val.&#40;p&#41;&#43;2err.&#40;p&#41;, color&#61;&quot;red&quot;, label&#61;&quot;upper bound&quot;&#41;
plot&#33;&#40;val.&#40;p&#41;-2err.&#40;p&#41;, color&#61;&quot;red&quot;, label&#61;&quot;lower bound&quot;&#41;</code></pre>
<p><img src="../../images/2019_julia/output_84_0.png" alt="plot" /></p>
<p>We see that the noise has been filtered and the error has been processed appropriately. Note that the region where the signal is 1, the uncertainty is really low. This corresponds to one of the flat regions of the sigmoid. When the value of <code>z</code> is large, the output is close to one, irregardless of noise.</p>
<p>Admittedly, this example seems a bit contrived. However, when you think of <code>z</code> as the output of some neural network, this might make some more sense, especially if you image performing binary classification on some space. For example, detecting vegetation in remote sensing data. The methodology that we have developed is not that far from a Bayesian artificial neural network.</p>
<p>The elephant in the room here is that by doing this convolution, the measurements are no longer independent, which leads to a serious underestimation of the uncertainty&#33; Implementing measurements which also keep track of their underlying correlations is left as an exercise for the reader.</p>
<div class="page-foot">
  <div class="copyright">
    &copy; Michiel Stock. Last modified: July 28, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
        </div> <!-- end of id=main -->
    </div> <!-- end of id=layout -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
